{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "원티드 프리온보딩 코스.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNv8PMHrnCWOdfUXWcXR241",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdding/wanted_pre_onboarding/blob/main/%EC%9B%90%ED%8B%B0%EB%93%9C_%ED%94%84%EB%A6%AC%EC%98%A8%EB%B3%B4%EB%94%A9_%EC%BD%94%EC%8A%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8e3TzVu8dkL7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "\n",
        "  def preprocessing(self, sequences):\n",
        "    result= []\n",
        "    for text in sequences:\n",
        "      result.append(text_to_word_sequence(text.lower()))\n",
        "    \n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    \n",
        "    i = 1\n",
        "    for token in tokens:\n",
        "      for t in token:\n",
        "        if t not in self.word_dict:\n",
        "          self.word_dict[t] = i\n",
        "          i += 1\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "  def transform(self, sequences):\n",
        "    tokens = self.preprocessing(sequences)\n",
        "\n",
        "    result = []\n",
        "    if self.fit_checker:\n",
        "      for token in tokens:\n",
        "        L = []\n",
        "        for t in token:\n",
        "          if t not in self.word_dict:   # 어휘 사전에 없는 단어는 'oov'의 index 0 으로 변환\n",
        "            L.append(0)\n",
        "          else:\n",
        "            L.append(self.word_dict[t])\n",
        "        result.append(L)\n",
        "          \n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "o_peuUwghuqs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenizer().fit_transform(['I go to school.', 'I LIKE pizza!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWVKhK1ViNPa",
        "outputId": "5adf744e-e025-4f3a-af4d-36b85b00fc99"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "\n",
        "    M = max(map(max, tokenized))   # 토큰 숫자중 최대값\n",
        "    IDF = []\n",
        "    \n",
        "    for i in range(1, M+1):\n",
        "      count = 0\n",
        "      for tokens in tokenized:\n",
        "        if i in tokens:\n",
        "          count += 1\n",
        "      IDF.append(np.log(len(tokenized) / (1+count)))  # IDF 값\n",
        "    \n",
        "    self.fit_checker = True\n",
        "    return IDF\n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.Tokenizer().transform(sequences)\n",
        "\n",
        "      self.IDF = self.fit()     # IDF 리스트 받아오기\n",
        "      M = max(map(max, tokenized))\n",
        "      TF = []\n",
        "      for tokens in tokenized:\n",
        "        count = []\n",
        "        for i in range(1, M+1):\n",
        "          count.append(tokens.count(i))\n",
        "        TF.append(count)\n",
        "        \n",
        "      tfidf_matrix = np.dot(TF, self.IDF)\n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "EGND3k9yo9a2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TfidfVectorizer(Tokenizer()).fit_transform(['I go to school.', 'I LIKE pizza!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "qQl1XuYI53ms",
        "outputId": "4460f315-462c-4c03-9ed4-708ae9e2e844"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-9f803563b79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I go to school.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I LIKE pizza!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-e1ff70b96dbd>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-e1ff70b96dbd>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_checker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# IDF 리스트 받아오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'Tokenizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = Tokenizer().fit_transform(['I go to school.', 'I LIKE pizza!'])\n",
        "M = max(map(max, tokenized))   # 토큰 숫자중 최대값\n",
        "IDF = []\n",
        "i = 1\n",
        "while i <= M:\n",
        "  count = 0\n",
        "  for tokens in tokenized:\n",
        "    if i in tokens:\n",
        "      count += 1\n",
        "  IDF.append(np.log(len(tokenized) / (1+count)))\n",
        "  i += 1 \n",
        "print(IDF)\n",
        "\n",
        "\n",
        "TF = []\n",
        "      \n",
        "for tokens in tokenized:\n",
        "  count = []\n",
        "  for i in range(1, M+1):\n",
        "    count.append(tokens.count(i))\n",
        "  TF.append(count)\n",
        "print(TF)\n",
        "\n",
        "\n",
        "tfidf_matrix = np.dot(TF, IDF)\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uPRANPsusvB",
        "outputId": "3dfc1cdc-33e4-49ae-f0c7-42981da9278a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[[1, 1, 1, 1, 0, 0], [1, 0, 0, 0, 1, 1]]\n",
            "[-0.40546511 -0.40546511]\n"
          ]
        }
      ]
    }
  ]
}